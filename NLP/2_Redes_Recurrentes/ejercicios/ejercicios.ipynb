{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxYVuQiI_uxO"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/institutohumai/cursos-python/blob/master/NLP/2_Redes_Recurrentes/ejercicios/ejercicios.ipynb\"> <img src='https://colab.research.google.com/assets/colab-badge.svg' /> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGUGih8T60cr"
   },
   "source": [
    "# Ejercicios clase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gc419sbFyhp"
   },
   "source": [
    "# 1 - Análisis de sentimiento simple\n",
    "\n",
    "En este práctico, crearemos un modelo de aprendizaje automático para detectar sentimientos (es decir, detectar si una oración es positiva o negativa) usando PyTorch y TorchText. Esto se hará en reseñas de películas, utilizando el [conjunto de datos de IMDb](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Comenzaremos de manera muy simple para comprender los conceptos generales sin preocuparnos realmente por los buenos resultados. Las siguientes secciones se basarán en estos conceptos y obtendremos buenos resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgW0OebUDadk"
   },
   "source": [
    "### Introducción\n",
    "\n",
    "Usaremos una **red neuronal recurrente** (RNN), ya que se usan comúnmente en el análisis de secuencias. Un RNN toma una secuencia de palabras, $X = \\{x_1, ..., x_T \\}$, una a la vez, y produce un _estado oculto_, $h$, para cada palabra. Usamos el RNN _recurrentemente_ introduciendo la palabra actual $x_t$ así como el estado oculto de la palabra anterior, $h_{t-1}$, para producir el siguiente estado oculto, $h_t$.\n",
    "\n",
    "$$h_t = \\text{RNN}(x_t, h_{t-1})$$\n",
    "\n",
    "Una vez que tenemos nuestro estado oculto final, $h_T$, (a partir de la última palabra en la secuencia, $x_T$) lo alimentamos a través de una capa lineal, $f$, (también conocida como capa densa), para recibir nuestro sentimiento predicho, $\\hat{y} = f(h_T)$.\n",
    "\n",
    "A continuación se muestra una oración de ejemplo, con la RNN prediciendo cero, lo que indica un sentimiento negativo. La RNN se muestra en naranja y la capa lineal se muestra en plateado. Tenga en cuenta que usamos la misma RNN para cada palabra, es decir, tiene los mismos parámetros. El estado oculto inicial, $ h_0 $, es un tensor inicializado a todos ceros.\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment1.png?raw=1)\n",
    "\n",
    "**Nota:** algunas capas y pasos se han omitido del diagrama, pero se explicarán más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE_hTt_RiUuU"
   },
   "source": [
    "Lo primero que haremos es dejar una semilla por defecto para los generadores de números aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt_WATfwFyhu"
   },
   "outputs": [],
   "source": [
    "#Setup\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wALYs4TxFyhs"
   },
   "source": [
    "## Preparando los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_pZbD8KIEbE"
   },
   "source": [
    "### Descarga de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1TqYZG6IgzX"
   },
   "source": [
    "Lo primero que haremos será instalar el modulo `torchdata` que contiene algunos datasets comunes en deep learning. \n",
    "\n",
    "> NOTA: Luego de ejecutar esta celda es necesario reiniciar el entorno de Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8Ow3jIZIas5",
    "outputId": "d42e50c1-5fbc-4e88-cdb0-36649c8d7290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.5.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKEP_Yo-im_-"
   },
   "source": [
    "A continuación descargaremos los datos de IMDB que usaremos para el modelo. `torchtext` tiene acceso a estos datos y los llamaremos con la clase correspondiente. Si bien la clase `IMDB` genera un `DataLoader`, vamos a hacer modificaciones para crear un `DataLoader` personalizado. Más adelante explicaremos nuestros argumentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myUGQPJGFyhw"
   },
   "outputs": [],
   "source": [
    "import torchdata\n",
    "import random\n",
    "from torchtext.datasets import IMDB\n",
    "train_data, test_data = IMDB(split=('train', 'test'))\n",
    "\n",
    "# guardaremos momentaneamente los datos en listas.\n",
    "full_list = list(train_data)\n",
    "cutoff = int(len(full_list) * 0.7)\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(full_list)\n",
    "\n",
    "train_list = full_list[:cutoff] # entrenamiento\n",
    "val_list = full_list[cutoff:] # validacion\n",
    "test_list = list(test_data) # prueba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-LPUGIHIQdz"
   },
   "source": [
    "### Armado de vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJqVa7ui0dO"
   },
   "source": [
    "Luego usaremos el tokenizador de spacy `en_core_web_sm` para crear la representación de las palabras de nuestro texto. Es decir, definiremos que un *token* es igual a una palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "me5RqNgkgEXP"
   },
   "outputs": [],
   "source": [
    "#@markdown # Ejercicio 1\n",
    "\n",
    "# inserte su código\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qquO6WLnjdJS"
   },
   "source": [
    "A continuación generaremos un vocabulario de `torchtext` como lo hicimos en clase. \n",
    "\n",
    "Para ello tenga en cuenta lo siguiente: \n",
    "\n",
    "* No incluiremos las palabras que se repitan menos de 10 veces. Para ello usaremos el argumento `min_freq` de la clase `vocab`.\n",
    "* También añadimos 2 tokens adicionales: `<unk>` para palabras desconocidas y `<PAD>` como \"relleno\". El token de relleno nos permite que todas las oraciones tengan igual número de *tokens*. Para ello usaremos el argumento llamado `specials` de la clase `vocab` al que deberemos pasarle la siguiente tupla `('<unk>', '<PAD>')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNYRP9Eag2AY"
   },
   "outputs": [],
   "source": [
    "#@markdown # Ejercicio 2\n",
    "# inserte su codigo aqui\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "# inserte su codigo aqui\n",
    "vocab = None #no cambie el nombre de esta variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cA9dhwL3EmR_"
   },
   "source": [
    "La siguiente linea le informa a vocab cual es el *token* que de usar al encontrar una palabra desconocida. Por eso incluimos ese token especial en la celda anterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_38R1FC2Ehy5"
   },
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GibgHLUIFCL8"
   },
   "source": [
    "Veamos algunos ejemplos de los métodos que ofrece la clase vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02derfgmg3FL",
    "outputId": "66bdbc5b-f505-41f1-af20-dd22c3d3118e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de tokens en el vocabulario es: 18961\n",
      "El índice del token 'the' es 10\n",
      "El token con indice 2 es Not\n",
      "A la palabra 'AYAYA' le corresponde el token <unk>\n"
     ]
    }
   ],
   "source": [
    "print(\"La cantidad de tokens en el vocabulario es:\", len(vocab))\n",
    "s = 'the'\n",
    "print(f\"El índice del token '{s}' es {vocab[s]}\")\n",
    "itos = vocab.get_itos()\n",
    "i = 2\n",
    "print(f\"El token con indice {i} es {itos[i]}\")\n",
    "s = \"AYAYA\"\n",
    "print(f\"A la palabra '{s}' le corresponde el token {itos[vocab[s]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJygipMmRkWc"
   },
   "source": [
    "### Generación de `Dataloader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6yfOUDdt-E2"
   },
   "source": [
    "El paso final para preparar los datos es crear los iteradores. Iteramos sobre estos en el ciclo de entrenamiento/evaluación, y devuelven un lote de ejemplos (indexados y convertidos en tensores) en cada iteración.\n",
    "\n",
    "Usaremos una clase diseñada para devolver un lote de ejemplos con longitudes similares, minimizando la cantidad de relleno por ejemplo.\n",
    "\n",
    "La nueva clase heredará de `Sampler` que es una clase de `torch`. `Sampler` se utiliza para indicarles a los `DataLoader` los índices que debe usar para armar cada minilote. Se necesita redefinir al menos 2 métodos: `__iter__` para armar un generador, `__len__` para saber la cantidad de elementos. Además crearemos un constructor `__init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njqYBX5CGdn0"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, batch_size, train_list):\n",
    "        self.length = len(train_list)\n",
    "        self.train_list = train_list\n",
    "        self.batch_size = batch_size\n",
    "        indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(self.train_list)]\n",
    "        random.seed(SEED)\n",
    "        random.shuffle(indices)\n",
    "        pooled_indices = []\n",
    "        # creamos minilotes de tamaños similares\n",
    "        for i in range(0, len(indices), self.batch_size * 100):\n",
    "            pooled_indices.extend(sorted(indices[i:i + self.batch_size * 100], key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        self.pooled_indices = pooled_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.pooled_indices), self.batch_size):\n",
    "            yield [idx for idx, _ in self.pooled_indices[i:i + self.batch_size]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "        return (self.length + self.batch_size - 1) // self.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni5ADvh1rl8W"
   },
   "source": [
    "Además tendremos definir una función `collate` que le indique al `DataLoader` los datos a incluir en cada minilote.\n",
    "\n",
    "Para ello crearemos dos funciones anónimas. Una convertirá nuestro texto a números que podamos manejar en nuestras redes. La otra convertirá nestras etiquetas en números. Como solo tenemos 2 clases: reseña negativa y reseña positiva. Les asignaremos valores 0 y 1 respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZ_WmLTEGA7n",
    "outputId": "32e8adb9-0b82-4e83-d378-6057c9899bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de entrada para text_transform: here is an example\n",
      "Ejemplo de entrada para text_transform: [331, 44, 121, 1064]\n"
     ]
    }
   ],
   "source": [
    "text_transform = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "s = \"here is an example\"\n",
    "print(\"Ejemplo de entrada para text_transform:\", s)\n",
    "print(\"Ejemplo de entrada para text_transform:\", text_transform(s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWXZPhnkGrP0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        # convertimos la etiqueta usando label_transform\n",
    "        label_list.append(label_transform(_label))\n",
    "        # convertimos el texto en tokens\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "        # guardamos la longitud de cada token\n",
    "        length_list.append(len(processed_text))\n",
    "    # armamos la tupla que conformara un ejemplo de minilote.\n",
    "    result = (torch.tensor(label_list),\n",
    "              pad_sequence(text_list, padding_value=1.0),\n",
    "              torch.tensor(length_list) )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yvZ74PXirXe"
   },
   "source": [
    "> NOTA: hemos incluido la longitud de la oración, porque es algo que necesitaremos en la segunda parte de nuestro trabajo práctico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hsrn3eynzCmg"
   },
   "source": [
    "Ahora estamos listos para generar los `DataLoader` que necesitamos.\n",
    "\n",
    "Para ello tenga encuenta que: \n",
    "\n",
    "* El `batch_size` deberá ser de 64 elementos\n",
    "* Debera crear un `DataLoader` para entrenamiento, validación y prueba. También para cada uno deberá crear su instancia de `BucketSampler`\n",
    "* En cada constructor deberá definir la función `collate` a utilizar con el atributo `collate_fn`.\n",
    "* También deberá definir el *sampler* a utilizar con el atributo `batch_sampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjgYAmlfFyh8"
   },
   "outputs": [],
   "source": [
    "#@markdown # Ejercicio 3\n",
    "\n",
    "# inserte su código aquí\n",
    "# no cambie los nombres de las siguientes variables\n",
    "train_bucket = None\n",
    "train_iter = None\n",
    "val_bucket = None\n",
    "val_iter = None\n",
    "test_bucket = None\n",
    "test_iter = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP1P0JZdggBA"
   },
   "source": [
    "## Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5-x66lDghoP"
   },
   "source": [
    "### Arquitectura de nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhUiPqsxFyh8"
   },
   "source": [
    "\n",
    "\n",
    "La siguiente etapa es construir el modelo que eventualmente vamos a entrenar y evaluar.\n",
    "\n",
    "Nuestra clase `Vanilla_RNN` es una subclase de` nn.Module`.\n",
    "\n",
    "Dentro del `__init__` definimos las _capas_ del módulo. Nuestras tres capas son una capa _embedding_, nuestra RNN y una capa _densa_. Todas las capas tienen sus parámetros inicializados a valores aleatorios, a menos que se especifique explícitamente.\n",
    "\n",
    "La capa de embedding se utiliza para transformar nuestro vector ralos one-hot (ralos ya que la mayoría de los elementos son 0) en un vector de embedding denso (denso ya que la dimensionalidad es mucho más pequeña y todos los elementos son números reales). Esta capa de embedding es simplemente una capa densa única. Además de reducir la dimensionalidad de la entrada al RNN, existe la teoría de que las palabras que tienen un impacto similar en el sentimiento de la revisión se mapean juntas en este denso espacio vectorial. Para obtener más información sobre los embeddings de palabras, consulte [aquí](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/).\n",
    "\n",
    "La capa RNN es nuestra RNN que toma nuestro vector denso y el estado oculto anterior $h_{t-1}$, y lo usa para calcular el siguiente estado oculto, $ h_t $.\n",
    "\n",
    "![](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/assets/sentiment7.png?raw=1)\n",
    "\n",
    "Finalmente, la capa linear toma el estado oculto final y lo alimenta a través de una capa densa, $ f(h_T) $, transformándola a la dimensión de salida correcta.\n",
    "\n",
    "Se llama al método `forward` cuando introducimos ejemplos en nuestro modelo.\n",
    "\n",
    "Cada lote, `text`, es un tensor de tamaño **[longitud de la oración, tamaño del lote]**. En un lote de oraciones, cada oración tiene sus palabras convertidas en un vector one-hot.\n",
    "\n",
    "Puede notar que este tensor debería tener otra dimensión debido al tamaño de los vectores one-hot. Sin embargo PyTorch almacena convenientemente un vector one-hot como su valor de índice. Es decir, el tensor que representa una oración, es solo un tensor de los índices para cada token en esa oración. El acto de convertir una lista de tokens en una lista de índices se denomina comúnmente *numeralización*.\n",
    "\n",
    "El lote de entrada luego se pasa a través de la capa de embedding lo que nos da una representación vectorial densa de nuestras oraciones. `embedded` es un tensor de tamaño **[longitud de la oración, tamaño del lote, embedding_dim]**.\n",
    "\n",
    "`embedded` se introduce luego en la RNN. En algunos frameworks, debemos alimentar el estado oculto inicial, $ h_0 $, en la RNN. Sin embargo, en PyTorch, si no se pasa un estado oculto inicial como argumento, se establece de forma predeterminada en un tensor de todos ceros.\n",
    "\n",
    "El RNN devuelve 2 tensores, `output` de tamaño **[longitud de la oración, tamaño de lote, dimensión de la variable oculta]** y` hidden` de tamaño **[1, tamaño de lote, dimensión de la variable oculta]**. `output` es la concatenación del estado oculto de cada paso de tiempo, mientras que ` hidden` es simplemente el estado oculto final. \n",
    "\n",
    "Finalmente, alimentamos el último estado oculto, `hidden`, a través de la capa lineal, `linear`, para producir una predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRiYnNxpglif"
   },
   "source": [
    "### Implementación de nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT4m0-T8gC_m"
   },
   "source": [
    "A continuación, deberá crear la clase `Vanilla_RNN`.\n",
    "\n",
    "Para ello tenga encuenta que debera:\n",
    "\n",
    "* definir un constructor, `__init__` con tres capas:\n",
    "  * Una instancia de `Embedding`\n",
    "  * Una instancia de `RNN`\n",
    "  * Una instancia de `Linear`\n",
    "\n",
    "* Recuerde también que:\n",
    "  * La dimensión de entrada de nuestra red es  la dimensión de los vectores one-hot. Es decir, es el tamaño del vocabulario.\n",
    "  * La dimensión a la salida de `Embedding` es el tamaño de los vectores de palabras densas. Suele tener entre 50 y 250 dimensiones, pero depende del tamaño del vocabulario.\n",
    "  * La dimensión del estado oculto es el tamaño de las variables ocultas. Suele rondar entre 100 y 500 dimensiones, pero también depende de factores como el tamaño del vocabulario, el tamaño de los vectores densos y la complejidad de la tarea.\n",
    "  * La dimensión de salida suele ser el número de clases, sin embargo, en el caso de solo 2 clases, el valor de salida está entre 0 y 1 y, por lo tanto, puede ser unidimensional, es decir, un solo número real escalar.\n",
    "\n",
    "* definir un método `forward`. Este método debe tomar nuestro texto y hacer lo siguiente:\n",
    "  1. Aplicar la capa `Embeddings` a nuestras oraciones.\n",
    "  2. Aplicar la capa `RNN` a los datos salidos de `Embeddings`\n",
    "  3. Aplicar una capa densa `Linear` al estado oculto final de la `RNN`\n",
    "  >NOTA: El estado oculto de RNN es de la forma: \n",
    "  `hidden = [ 1, batch size, hid dim ]`. Es posible que necesite aplicar un `squeeze` en la dimensión 0 para eliminar ese 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVmXcKVkFyh9"
   },
   "outputs": [],
   "source": [
    "#@markdown # Ejercicio 4\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Vanilla_RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "      # Inserte su código aquí.\n",
    " \n",
    "        \n",
    "    def forward(self, text):\n",
    "      # Inserte su código aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFCEMwZ3Fyh9"
   },
   "source": [
    "Ahora creamos una instancia de nuestra clase RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4LhijkrFyh9"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model1 = Vanilla_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLAA1l7bFyh9"
   },
   "source": [
    "También creemos una función que nos diga cuántos parámetros entrenables tiene nuestro modelo para que podamos comparar el número de parámetros en diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5ADp7jxFyh-",
    "outputId": "d1036a5a-b207-41c6-f273-e5f734db87c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 1,988,005 parámetros entrenables\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'El modelo tiene {count_parameters(model1):,} parámetros entrenables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul_5J7R_Fyh-"
   },
   "source": [
    "## Entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh-CXICmFyh-"
   },
   "source": [
    "Ahora vamos a configurar el entrenamiento y a entrenar el modelo.\n",
    "\n",
    "Primero, crearemos un optimizador. Este es el algoritmo que usamos para actualizar los parámetros del módulo. Aquí, usaremos _descenso de gradiente estocástico_ (SGD). El primer argumento son los parámetros que serán actualizados por el optimizador y el segundo es la tasa de aprendizaje, es decir, cuánto cambiaremos los parámetros cuando hagamos una actualización de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiT_6M0hFyh-"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPmiIiP7Fyh_"
   },
   "source": [
    "A continuación, definiremos nuestra función de pérdida. \n",
    "\n",
    "La función de pérdida aquí es _entropía cruzada binaria con logits_.\n",
    "\n",
    "Nuestro modelo genera actualmente un número real sin consolidar. Como nuestras etiquetas son 0 o 1, queremos restringir las predicciones a un número entre 0 y 1. Hacemos esto usando las funciones _sigmoidea_.\n",
    "\n",
    "Luego usamos este escalar para calcular la pérdida usando entropía cruzada binaria.\n",
    "\n",
    "La clase `BCEWithLogitsLoss` lleva a cabo los pasos de entropía cruzada sigmoidea y binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "429y_6BkFyh_"
   },
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3Nz1Os8Fyh_"
   },
   "source": [
    "Usando `.to`, podemos ubicar al modelo y la función de pérdida en el GPU (si tenemos alguno). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1corCOUXFyh_"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model1 = model1.to(device)\n",
    "loss = loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsgcrZkTFyiA"
   },
   "source": [
    "Ya establecimos cómo la pérdida, sin embargo, tenemos que escribir nuestra función para calcular el accuracy.\n",
    "\n",
    "Esta función primero alimenta las predicciones a través de una capa sigmoidea, reescalando los valores entre 0 y 1, luego los redondeamos al número entero más cercano. Esto redondea cualquier valor superior a 0,5 a 1 (un sentimiento positivo) y el resto a 0 (un sentimiento negativo).\n",
    "\n",
    "Luego calculamos cuántas predicciones redondeadas son iguales a las etiquetas reales y la promediamos en todo el lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nScl7_CGFyiA"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    # aproximamos al entera más cercano\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convertimos a flotante para la división\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKU52miSFyiA"
   },
   "source": [
    "La función `train_epoch` itera sobre todos los ejemplos, un lote a la vez.\n",
    "\n",
    "Alimentamos el lote de oraciones, `text`, en el modelo. Es necesario usar la función `squeeze` ya que las predicciones tienen inicialmente la forma **[tamaño de lote, 1]**, y necesitamos eliminar la dimensión de tamaño 1 ya que PyTorch espera que la entrada de predicciones a nuestra función de pérdida tenga forma **[tamaño del lote]**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f269HhVqFyiB"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for label, text, _ in iterator:\n",
    "        label = label.float().to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "    return epoch_loss * batch_size / len(iterator), epoch_acc * batch_size / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nsv_u71tFyiB"
   },
   "source": [
    "`evaluate_epoch` es parecida a `train_epoch`, con algunas modificaciones ya que no desea actualizar los parámetros al evaluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXHxKDsAFyiB"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for label, text, _ in iterator:\n",
    "          \n",
    "            label = label.float().to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss * batch_size/ len(iterator), epoch_acc * batch_size / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GA6H7qbFyiB"
   },
   "source": [
    "También vamos a crear una función para decirnos cuánto tarda una época en entrenarse para comparar los tiempos de entrenamiento entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddWoeFYpFyiC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWjnFd7MFyiC"
   },
   "source": [
    "Luego entrenamos el modelo a través de múltiples épocas, una época es un pase completo a través de todos los ejemplos en los conjuntos de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JBDTNHlFyiC",
    "outputId": "fa330afe-9409-4da9-d635-c24e3ae215ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.824 | Train Acc: 49.93%\n",
      "\t Val. Loss: 0.722 |  Val. Acc: 50.08%\n",
      "Epoch: 02 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.822 | Train Acc: 51.58%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 51.56%\n",
      "Epoch: 03 | Epoch Time: 0m 22s\n",
      "\tTrain Loss: 0.707 | Train Acc: 56.49%\n",
      "\t Val. Loss: 0.879 |  Val. Acc: 50.90%\n",
      "Epoch: 04 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: 0.760 | Train Acc: 54.89%\n",
      "\t Val. Loss: 0.760 |  Val. Acc: 51.36%\n",
      "Epoch: 05 | Epoch Time: 0m 21s\n",
      "\tTrain Loss: nan | Train Acc: 0.21%\n",
      "\t Val. Loss: nan |  Val. Acc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model1, train_iter,\n",
    "                                        optimizer1, loss, device)\n",
    "    valid_loss, valid_acc = evaluate_epoch(model1, val_iter, loss, device)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnZWN0kdFyiD"
   },
   "source": [
    "Aunque la perdida cae, la precisión es deficiente. Esto se debe a varios problemas con el modelo que mejoraremos a continuación.\n",
    "\n",
    "Por último veamos la pérdida de nuestro modelo con el data ser de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjQrmx5ZFyiE",
    "outputId": "9af42408-d404-4a2d-ccc9-1f72aeafccfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: nan | Test Acc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_epoch(model1, test_iter, loss, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8T3dLGzLLSTU"
   },
   "source": [
    "# 2 - Análisis de Sentimiento Potenciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSzBc_zfFvaQ"
   },
   "source": [
    "## Preparando los datos\n",
    "\n",
    "Usaremos **secuencias empaquetadas con padding**, lo que hará que nuestro RNN solo procese los tokens distintos a `<pad>` de nuestra secuencia, y para cualquier elemento `<pad>`, la \"salida\" será un tensor cero. Para usar secuencias empaquetadas con padding, tenemos que decirle al RNN la longitud de las secuencias reales. Afortunadamente, en la creación de nuestros `DataLoader` la función `collate` ya se encargaba de esto. La principal diferencia la tendremos en nuestro modelo, que deberá tomar recibir como entrada la longitud de la oración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUae6yInGj9K"
   },
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9u69MfxFvbv"
   },
   "source": [
    "###  Una arquitectura RNN diferente\n",
    "\n",
    "En esta segunda parte, usaremos una arquitectura RNN diferente. Para ello usaremos podemos elegir entre una LSTM o una GRU. ¿Por qué estás son mejores que una RNN estándar? Los RNN estándar sufren el [problema del gradiente que se desvanece](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). LSTM y GRU superan esto al usar de múltiples **_compuertas_** que controlan el flujo de información dentro y fuera de la memoria.\n",
    "\n",
    "Recuerde que LSTM devuelve la celda de memoria y el estado oculto. Por lo cual debera manejar esa diferencia. Sin embargo, la predición del sentimiento todavía se realiza utilizando la variable oculta final, no el estado final de la celda, es decir, $ \\hat {y} = f (h_T) $. Para ahorrarnos estos problemas, decidimos usar `GRU`\n",
    "\n",
    "###  RNN Bidireccional\n",
    "\n",
    "El concepto detrás de una RNN bidireccional es simple. Además de tener una RNN procesando las palabras en la oración desde la primera hasta la última (una RNN hacia adelante), tenemos una segunda RNN procesando las palabras en la oración desde **la última a la primera** (una RNN hacia atrás) . En el tiempo $ t $, la RNN hacia adelante está procesando la palabra $ x_t $, y la RNN hacia atrás está procesando la palabra $ x_{T-t + 1} $.\n",
    "\n",
    "En PyTorch, los tensores de las variables ocultas (y los estados de celda) devueltos por las RNN hacia adelante y hacia atrás se apilan uno encima del otro en un solo tensor.\n",
    "\n",
    "Hacemos nuestra predicción de sentimiento usando una concatenación del último estado oculto de la RNN hacia adelante (obtenido de la palabra final de la oración), $ h_T ^ \\rightarrow $, y el último estado oculto de la RNN hacia atrás (obtenido a partir de la primera palabra de la oración), $ h_T ^ \\leftarrow $, es decir, $ \\hat {y} = f (h_T ^ \\rightarrow, h_T ^ \\leftarrow) $\n",
    "\n",
    "La siguiente imagen muestra una RNN bidireccional, con la RNN hacia adelante en naranja, la RNN hacia atrás en verde y la capa lineal en plateado.\n",
    "\n",
    "![](https://i.imgur.com/g1mAJXt.png)\n",
    "\n",
    "### RNN Multicapa\n",
    "\n",
    "Las RNN multicapa (también llamados *RNN profundas*) son otro concepto simple. La idea es que agreguemos RNN adicionales sobre la RNN estándar inicial, donde cada RNN agregado es otra *capa*. La salida de la variable oculta de la primera RNN (inferior) en el tiempo $ t $ será la entrada de la RNN por encima de ella en el paso de tiempo $ t $. Luego, la predicción se realiza a partir de la variable oculta final de la capa final (más alta).\n",
    "\n",
    "La siguiente imagen muestra una RNN unidireccional multicapa, donde el número de capa se da como superíndice. También tenga en cuenta que cada capa necesita su propia variable oculta inicial, $ h_0 ^ L $.\n",
    "\n",
    "![](https://i.imgur.com/wBKSBhx.png)\n",
    "\n",
    "### Regularización\n",
    "\n",
    "Aunque hemos agregado mejoras a nuestro modelo, cada una de ellas agrega parámetros adicionales. Cuantos más parámetros tenga en su modelo, mayor será la probabilidad de que su modelo se sobreajuste (memorice los datos de entrenamiento, lo que provoca un error de entrenamiento bajo pero un error de validación/prueba alto, es decir, una generalización deficiente para ejemplos nuevos e inéditos). \n",
    "\n",
    "Para combatir esto, usamos la regularización. Más específicamente, usamos un método de regularización llamado **dropout**. El dropout funciona al *eliminar* (configurando en 0) neuronas en una capa al azar durante un pase hacia adelante. La probabilidad de que se descarte cada neurona se establece mediante un hiperparámetro y cada neurona con dropout aplicado se considera de forma independiente. Una teoría sobre por qué funciona el dropout es que un modelo con parámetros abandonados puede verse como un modelo \"más simple\" (menos parámetros). Las predicciones de todos estos modelos \"más simples\" (una para cada pase hacia adelante) se promedian juntas dentro de los parámetros del modelo. Por lo tanto, su único modelo puede considerarse como un conjunto de modelos más simples, ninguno de los cuales está sobre parametrizado y, por lo tanto, no debe sobreajustarse.\n",
    "\n",
    "### Detalles de Implementación\n",
    "\n",
    "Otra adición a este modelo es que no vamos a aprender el embedding del token `<pad>`. Esto se debe a que queremos decirle explícitamente a nuestro modelo que los tokens de relleno son irrelevantes para determinar el sentimiento de una oración. Esto significa que el embedding del token de padding permanecerá como se inicializó (todos ceros). Hacemos esto pasando el índice de nuestro token `<pad>` como el argumento `padding_idx` a la capa` nn.Embedding`.\n",
    "\n",
    "Para usar un GRU en lugar del RNN estándar, usamos `nn.GRU` en lugar de` nn.RNN`. Además, tenga en cuenta que el LSTM devuelve la \"salida\" y una tupla con el estado final de la variable oculta y de la \"celda\", mientras que el RNN estándar solo devuelve la \"salida\" y el estado final de la variable oculta.\n",
    "\n",
    "Como el estado oculto final de nuestro GRU tiene un componente hacia adelante y hacia atrás, que se concatenarán juntos, el tamaño de la entrada a la capa `nn.Linear` es el doble que el tamaño de la dimensión oculta.\n",
    "\n",
    "La implementación de la bidireccionalidad y la adición de capas adicionales se realizan pasando valores para los argumentos `num_layers` y` bidirectional` para el LSTM.\n",
    "\n",
    "El dropout se implementa inicializando una capa `nn.Dropout` (el argumento es la probabilidad de descartar cada neurona) y usándola dentro del método` forward` después de cada capa a la que queremos aplicar el dropout. **Nota**: nunca use dropout en las capas de entrada o salida (`text` o` fc` en este caso), solo desea usarlo en las capas intermedias. Las LSTM tienes un argumento de \"dropout\" que añade dropout en las conexiones entre las variables ocultas en una capa y las variables ocultas en la siguiente capa.\n",
    "\n",
    "Como estamos pasando las longitudes de nuestras oraciones para poder usar secuencias empaquetadas con padding, tenemos que agregar un segundo argumento, `text_lengths`, a` forward`.\n",
    "\n",
    "Antes de pasar nuestras embeddings al RNN, necesitamos empaquetarlas, lo que hacemos con `nn.utils.rnn.packed_padded_sequence`. Esto hará que nuestro RNN solo procese los elementos no rellenados de nuestra secuencia. Entonces, el RNN devolverá `package_output` (una secuencia empaquetada) así como los estados` hidden` y `cell` (ambos son tensores). Sin secuencias empaquetadas con padding, ` hidden` es un tensor del último elemento de la secuencia, que probablemente será un token de relleno; sin embargo, cuando se utilizan secuencias empaquetadas con padding, ambos son del último elemento no rellenado de la secuencia. Tenga en cuenta que el argumento `lengths` de` package_padded_sequence` debe ser un tensor de CPU, por lo que lo convertimos explícitamente en uno usando `.to('cpu')`.\n",
    "\n",
    "Luego descomprimimos la secuencia de salida, con `nn.utils.rnn.pad_packed_sequence`, para transformarla de una secuencia empaquetada a un tensor. Los elementos de \"salida\" de los tokens de relleno serán tensores cero (tensores donde cada elemento es cero). \n",
    "\n",
    "El estado final de la variable oculta, `hidden`, tiene una forma de **[núm capas * núm direcciones, tamaño de lote, dimensión de la variable oculta]**. Estos están ordenados: **[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer 1, ..., forward_layer_n, backward_layer n]**. Queremos todas las variables ocultas en su versión hacia adelante y hacia atras para alimentar a la capa final. Para esto debemos usar la función `chunk` para recuparar cada variable oculta y luego `cat` para transformalas en un matriz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw7PyzY3Gcup"
   },
   "source": [
    "### Implementación de nuestro modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pahg2dFFGcur"
   },
   "source": [
    "A continuación, deberá crear la clase `RNN`.\n",
    "\n",
    "Para ello tenga encuenta que debera:\n",
    "\n",
    "* definir un constructor, `__init__` con tres capas:\n",
    "  * Una instancia de `Embedding`\n",
    "  * Una instancia de `GRU`\n",
    "  * Una instancia de `Linear`\n",
    "\n",
    "* Recuerde también que:\n",
    "  * La dimensión de entrada de nuestra red es la dimensión de los vectores one-hot. Es decir, es el tamaño del vocabulario.\n",
    "  * La dimensión a la salida de `Embedding` es el tamaño de los vectores de palabras densas. Suele tener entre 50 y 250 dimensiones, pero depende del tamaño del vocabulario.\n",
    "  * La instancia de `Embebbing` debe recibir 1 argumentos adicionales: `padding_idx`. Este argumento le indica al embedding que debe ignorar los rellenos.\n",
    "  * La dimensión del estado oculto es el tamaño de las variables ocultas. Suele rondar entre 100 y 500 dimensiones, pero también depende de factores como el tamaño del vocabulario, el tamaño de los vectores densos y la complejidad de la tarea.\n",
    "  * La instancia de GRU debe recibir 3 argumentos adicionales: `dropout`, `bidirectional`, `num_layers`\n",
    "  * La dimensión de salida suele ser el número de clases, sin embargo, en el caso de solo 2 clases, el valor de salida está entre 0 y 1 y, por lo tanto, puede ser unidimensional, es decir, un solo número real escalar.\n",
    "  * Como queremos las variables ocultas hacia adelante y hacia atras, debemos decirle a la capa lineal final que recibira un tensor de la forma `hidden_dim * num_layers * 2` para redes bidireccionales o `hidden_dim * num_layers` para redes no bidireccionales\n",
    "\n",
    "* definir un método `foward`. Este método tiene como entrada nuestro texto `text`, la longitud de nuestras oraciones `text_lengths`. Con esas entradas debe hacer lo siguiente:\n",
    "  1. Aplicar la capa `Embeddings` a nuestras oraciones, usando el argumento `padding_idx`\n",
    "  2. Aplicar un dropout la salida de la capa `Embeddings`\n",
    "  3. Ahora debemos empaquetar nuestra salida con `padding`. Para ello usaremos la función  `pack_padded_sequence` en el modulo `torch.nn.utils.rnn`. A este método debemos pasarle nuestra salida de `Embedding` con el dropout y la longitud del texto, `text_lengths`. Ademas necesitamos que `text_lengths`este en CPU.\n",
    "  2. Aplicar la capa `GRU` a los datos salidos de `Embeddings`, usando el argumento `dropout`\n",
    "  4. El argumento dropout de `GRU` aplica dropout a cada estado oculto, excepto al último. Por esto debemos agregar un dropout a la última variable oculta de `GRU`\n",
    "  5. Además debemos reestructurar nuestra salida de GRU. Tenemos que aplicar las funciones `chunk`,  `cat` y `squeeze` del modulo `torch` para pasar de  de un tensor de la forma:\n",
    "\n",
    "    `hidden = [num layers * num directions, batch size, hid dim]`\n",
    "\n",
    "    a otro de la forma \n",
    "\n",
    "    `hidden = [batch size, hid dim * num layers * num direction]`\n",
    "    \n",
    "    donde `num directions` es 2 para RNN bidireccionales o 1 para RNN convencionales.\n",
    "  3. Aplicar una capa densa `Linear` al estado oculto final de la `GRU`\n",
    "\n",
    "> NOTA: A la salida de GRU tenemos un tensor `output` que ha sido empaquetado. En este problema no necesitamos usar el tensor `output`, pero en otros problemas, ese tensor empaqutado puede ser util. Para desempaquetarlo existe la función `pad_packed_sequence(packed_output)` que devuelve las salidas desempaquetadas con su longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ll2w5F4VTspy"
   },
   "outputs": [],
   "source": [
    "#@markdown # Ejercicio 5\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, pad_index,\n",
    "                 hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "      # Inserte su código aquí.\n",
    "\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "      # Inserte su código aquí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTNDBzGUTspz"
   },
   "source": [
    "Ahora creamos una instancia de nuestra clase RNN.\n",
    "\n",
    "La dimensión de entrada es la dimensión de los vectores one-hot, que es igual al tamaño del vocabulario.\n",
    "\n",
    "La dimensión de embedding es el tamaño de los vectores de palabras densas. Suele tener entre 50 y 250 dimensiones, pero depende del tamaño del vocabulario.\n",
    "\n",
    "Como ya hemos creado un vocabulario, debemos informale a la red cual es nuestro token de relleno\n",
    "\n",
    "La dimensión oculta es el tamaño de las variables ocultas. Suele rondar entre 100 y 500 dimensiones, pero también depende de factores como el tamaño del vocabulario, el tamaño de los vectores densos y la complejidad de la tarea.\n",
    "\n",
    "Para activar la bidirecionalidad de una RNN en `torch` debemos pasar una variable `True` para que que el *framework* habilite la bidireccionalidad.\n",
    "\n",
    "Analogamente, debemos pasar el número de capas RNN, en este caso solo usaremos 2\n",
    "\n",
    "La dimensión de salida suele ser el número de clases, sin embargo, en el caso de solo 2 clases, el valor de salida está entre 0 y 1 y, por lo tanto, puede ser unidimensional, es decir, un solo número real escalar.\n",
    "\n",
    "Por último debemos decir cuanto dropout agregaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyy-_snMTsp1"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "PAD_IDX = vocab['<PAD>']\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model2 = RNN(INPUT_DIM, EMBEDDING_DIM, PAD_IDX,\n",
    "            HIDDEN_DIM,OUTPUT_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXPS2AC0Tsp1"
   },
   "source": [
    "También creemos una función que nos diga cuántos parámetros entrenables tiene nuestro modelo para que podamos comparar el número de parámetros en diferentes modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RQ3n8CgTsp2",
    "outputId": "e05ae800-8bf3-43da-f325-b85cc99ea338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo tiene 1,988,005 parámetros entrenables\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'El modelo tiene {count_parameters(model1):,} parámetros entrenables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCi8JAzLTsp3"
   },
   "source": [
    "## Entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R100Tvok4pwD"
   },
   "source": [
    "Reusaremos la funciones anteiores. La única diferencia es que debemos crear una nueva instancia de optimizador que aplique sobre nuestro nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtal3fh7Tsp4"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer2 = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpraCFJB453y"
   },
   "source": [
    "Además, debemos volver a a enviar nuestro modelo a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNaBF-I1Tsp6"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model2 = model2.to(device)\n",
    "loss = loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E1FETNJTsqA"
   },
   "source": [
    "Como nuestro nuevo modelo requiere de las longitudes de de las oraciones, debemos modificar la función de entrenamiento y de evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBeSfovyTsqB"
   },
   "outputs": [],
   "source": [
    "def train_epoch_wl(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    stop = 0\n",
    "    for label, text, length in iterator:\n",
    "        label = label.float().to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(text, length).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "    return epoch_loss * batch_size / len(iterator), epoch_acc * batch_size / len(iterator) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZwhy70M5UB5"
   },
   "source": [
    "Hacemos lo mismo con la otra función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4WanIdrTsqC"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch_wl(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        stop = 0\n",
    "        for label, text, length in iterator:\n",
    "          \n",
    "\n",
    "            label = label.float().to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            predictions = model(text, length).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss * batch_size / len(iterator), epoch_acc * batch_size / len(iterator) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1ouL0eyTsqD"
   },
   "source": [
    "Luego entrenamos el modelo a través de múltiples épocas, una época es un pase completo a través de todos los ejemplos en los conjuntos de entrenamiento y validación.\n",
    "\n",
    "En cada época, si la pérdida de validación es la mejor que hemos visto hasta ahora, guardaremos los parámetros del modelo y luego, una vez finalizado el entrenamiento, usaremos ese modelo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dA5t4ufnTsqE",
    "outputId": "1baf3846-7f6f-41d6-e264-e3a250e98221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.676 | Train Acc: 57.85%\n",
      "\t Val. Loss: 0.664 |  Val. Acc: 60.88%\n",
      "Epoch: 02 | Epoch Time: 0m 41s\n",
      "\tTrain Loss: 0.613 | Train Acc: 66.19%\n",
      "\t Val. Loss: 0.534 |  Val. Acc: 74.20%\n",
      "Epoch: 03 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.480 | Train Acc: 77.32%\n",
      "\t Val. Loss: 0.504 |  Val. Acc: 80.96%\n",
      "Epoch: 04 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.381 | Train Acc: 83.47%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 84.20%\n",
      "Epoch: 05 | Epoch Time: 0m 40s\n",
      "\tTrain Loss: 0.323 | Train Acc: 86.62%\n",
      "\t Val. Loss: 0.365 |  Val. Acc: 86.14%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch_wl(model2, train_iter,\n",
    "                                        optimizer2, loss, device)\n",
    "    valid_loss, valid_acc = evaluate_epoch_wl(model2, val_iter, loss, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1c1II3cTsqE",
    "outputId": "8c21f08c-edea-4496-a0b3-77b0f6020105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.368 | Test Acc: 85.46%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_epoch_wl(model2, test_iter, loss, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90QTJ0JNZznI"
   },
   "source": [
    "## Entrada del usuario\n",
    "\n",
    "Ahora podemos usar nuestro modelo para predecir el sentimiento de cualquier oración que le demos. Como ha sido entrenado en reseñas de películas, las oraciones proporcionadas también deben ser reseñas de películas.\n",
    "\n",
    "Cuando se usa un modelo para la predicción, siempre debe estar en modo de evaluación. Si se sigue este tutorial paso a paso, entonces ya debería estar en modo de evaluación (desde que ejecutamos `evaluate()` en el conjunto de prueba), sin embargo, lo configuramos explícitamente para evitar cualquier riesgo.\n",
    "\n",
    "Nuestra función `predict_sentiment` debe hacer algunas cosas:\n",
    "- configurar el modelo en modo de evaluación\n",
    "- tokenizar la oración, es decir, pasar de una cadena sin procesar a una lista de tokens\n",
    "- indexar los tokens convirtiéndolos en su representación entera de nuestro vocabulario\n",
    "- obtener la longitud de nuestra secuencia\n",
    "- convertir los índices, que son una lista de Python en un tensor de PyTorch\n",
    "- agregar una dimensión de lote al hacer `unsqueeze`\n",
    "- convertir la longitud de la secuencia en un tensor\n",
    "- rescalar la predicción de salida a un rango entre 0 y 1 con la función \"sigmoide\"\n",
    "- convertir el tensor que contiene un valor único en un número entero con el método `item()`\n",
    "\n",
    "Esperamos que las reseñas con un sentimiento negativo devuelvan un valor cercano a 0 y que las reseñas positivas devuelvan un valor cercano a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78TLVgUGZznK"
   },
   "outputs": [],
   "source": [
    "# @markdown # Ejercicio 6\n",
    "\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "  # inserte su código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXj7rfzQZznM"
   },
   "source": [
    "Un ejemplo de reseña negativa..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBoqyj6EZznN",
    "outputId": "87dc4b1e-aad4-488f-cf51-7cfeb8f5f548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014458732679486275"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model2, \"This movie is terrible\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO2VB7gDZznO"
   },
   "source": [
    "Un ejemplo de reseña positiva..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnCILO7_ZznP",
    "outputId": "31d2ae7d-5297-4527-e3df-bc2d0109e9dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371154427528381"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model2, \"Greatest film ever!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8gc419sbFyhp",
    "wALYs4TxFyhs",
    "H_pZbD8KIEbE",
    "N-LPUGIHIQdz",
    "eJygipMmRkWc",
    "rP1P0JZdggBA",
    "K5-x66lDghoP",
    "uRiYnNxpglif",
    "Ul_5J7R_Fyh-",
    "HSzBc_zfFvaQ",
    "xUae6yInGj9K",
    "F9u69MfxFvbv",
    "Sw7PyzY3Gcup"
   ],
   "name": "ejercicios",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
